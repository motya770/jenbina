{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c27234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4 streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ff6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "local_llm = 'llama3.2:3b-instruct-fp16'\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b689627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc98481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-2a00e9fdb996>:36: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  needs_chain = LLMChain(\n",
      "<ipython-input-4-2a00e9fdb996>:45: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = needs_chain.run(hunger_level=person.hunger)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an AI making decisions based on basic needs.\n",
      "Current hunger level: 94.01312516417863/100 (100 is full, 0 is starving)\n",
      "\n",
      "Based on this hunger level, what action should be taken? \n",
      "Respond in JSON format with two fields:\n",
      "- action: what to do (eat, find_food, or continue_activities)\n",
      "- reasoning: brief explanation why\n",
      "\n",
      "Consider:\n",
      "- Below 30: Critical need to find food\n",
      "- 30-60: Should consider eating soon\n",
      "- Above 60: Can continue other activities\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Current state: 94.01312516417863\n",
      "AI Decision: {\n",
      "    \"action\": \"find_food\",\n",
      "    \"reasoning\": \"Hunger level is at 94.01312516417863, which indicates a moderate to high need for sustenance. It's recommended to find food to maintain energy levels and prevent hunger from becoming too intense.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class BasicNeeds:\n",
    "    hunger: float = 100.0  # 100 is full, 0 is starving\n",
    "    \n",
    "    def update_needs(self):\n",
    "        # Simulate hunger decreasing over time\n",
    "        self.hunger = max(0, self.hunger - random.uniform(5, 15))\n",
    "        \n",
    "    def satisfy_hunger(self, amount: float):\n",
    "        self.hunger = min(100, self.hunger + amount)\n",
    "\n",
    "# Prompt template for decision making based on needs\n",
    "needs_prompt = PromptTemplate(\n",
    "    input_variables=[\"hunger_level\"],\n",
    "    template=\"\"\"You are an AI making decisions based on basic needs.\n",
    "Current hunger level: {hunger_level}/100 (100 is full, 0 is starving)\n",
    "\n",
    "Based on this hunger level, what action should be taken? \n",
    "Respond in JSON format with two fields:\n",
    "- action: what to do (eat, find_food, or continue_activities)\n",
    "- reasoning: brief explanation why\n",
    "\n",
    "Consider:\n",
    "- Below 30: Critical need to find food\n",
    "- 30-60: Should consider eating soon\n",
    "- Above 60: Can continue other activities\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for decision making\n",
    "needs_chain = LLMChain(\n",
    "    llm=llm_json_mode,\n",
    "    prompt=needs_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "person = BasicNeeds()\n",
    "person.update_needs()  # Simulate time passing\n",
    "response = needs_chain.run(hunger_level=person.hunger)\n",
    "print(f\"Current state: {person.hunger}\")\n",
    "print(f\"AI Decision: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f9d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are describing a world where a person lives in Small house in a Moldovan village. Ommit person feelings and thoughts.  \n",
      "    it is only describtion of environment and surroundings.\n",
      "Current time: morning\n",
      "Weather: sunny\n",
      "Hunger Level: 94.01312516417863/100\n",
      "\n",
      "Previous context:\n",
      "\n",
      "\n",
      "Describe the current situation and surroundings in two lists maintaining consistency with previous descriptions. :\n",
      "- list_of_descriptions: list of descriptions\n",
      "- list_of_actions: list of available actions\n",
      "The lists should be in JSON format.\n",
      "The list_of_descriptions can include following details:\n",
      "- The immediate environment (house, room, etc.)\n",
      "- Sensory details (smells, sounds, temperature)\n",
      "- Available resources or items nearby\n",
      "- Cultural context of the Moldovan village setting\n",
      "\n",
      "The list_of_actions should include available actions that can be taken by the person\n",
      "Keep the description coherent with previous ones and consider the person's current needs.\n",
      "\n",
      "Respond in JSON format with two fields:\n",
      "- list_of_descriptions: list of descriptions\n",
      "- list_of_actions: list of available actions\n",
      "- reasoning: brief explanation why\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\"list_of_descriptions\": [\"The small house is made of wood and has a thatched roof. The windows are small and let in plenty of sunlight on this sunny morning. The room is simple, with a wooden bed, a table, and a chair. There is a small fire pit in the corner, where ashes from last night's fire still smolder. The air is warm and filled with the smell of wood smoke and baking bread. Outside, the village is quiet, with only the occasional bird call breaking the silence. A wooden fence surrounds the house, with a gate that leads to a small garden. In the garden, there are several vegetables growing, including potatoes, carrots, and cabbage. The sun rises over the hills in the distance.\", \"The kitchen is equipped with a simple stove, where a pot of porridge is simmering on the heat. There is also a basket of bread, which was baked this morning. A jug of milk sits on the counter, next to a bowl of fresh eggs. The room is warm and cozy, with a fire crackling in the hearth.\", \"The village is surrounded by rolling hills and fields of wheat and barley. In the distance, there is a church steeple rising above the trees. The air is filled with the scent of freshly cut grass and the sound of birds singing. A small stream runs through the center of the village, providing water for the villagers.\"], \"list_of_actions\": [\"Cook porridge from the pot on the stove\", \"Bake bread in the oven\", \"Gather vegetables from the garden\", \"Go to the church to attend morning prayers\", \"Take a walk around the village and explore\", \"Check the fire pit for ashes to use as fuel\", \"Feed the chickens in the backyard\"], \"reasoning\": \"The person is hungry, with a hunger level of 94.01312516417863/100. The kitchen has porridge cooking on the stove, which can be cooked to satisfy their hunger. The garden also provides fresh vegetables that can be gathered and eaten. These options are available because the person's current needs are related to food and sustenance.\"}\n"
     ]
    }
   ],
   "source": [
    "# World state and description management\n",
    "@dataclass\n",
    "class WorldState:\n",
    "    location: str = \"Small house in a Moldovan village\"\n",
    "    time_of_day: str = \"morning\"\n",
    "    weather: str = \"sunny\"\n",
    "    last_descriptions: list = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.last_descriptions is None:\n",
    "            self.last_descriptions = []\n",
    "    \n",
    "    def add_description(self, description: str):\n",
    "        self.last_descriptions.append(description)\n",
    "        # Keep last 5 descriptions for context\n",
    "        if len(self.last_descriptions) > 5:\n",
    "            self.last_descriptions.pop(0)\n",
    "\n",
    "# Prompt template for world description\n",
    "world_prompt = PromptTemplate(\n",
    "    input_variables=[\"location\", \"time_of_day\", \"weather\", \"last_descriptions\", \"hunger_level\"],\n",
    "    template=\"\"\"You are describing a world where a person lives in {location}. Ommit person feelings and thoughts.  \n",
    "    it is only describtion of environment and surroundings.\n",
    "Current time: {time_of_day}\n",
    "Weather: {weather}\n",
    "Hunger Level: {hunger_level}/100\n",
    "\n",
    "Previous context:\n",
    "{last_descriptions}\n",
    "\n",
    "Describe the current situation and surroundings in two lists maintaining consistency with previous descriptions. :\n",
    "- list_of_descriptions: list of descriptions\n",
    "- list_of_actions: list of available actions\n",
    "The lists should be in JSON format.\n",
    "The list_of_descriptions can include following details:\n",
    "- The immediate environment (house, room, etc.)\n",
    "- Sensory details (smells, sounds, temperature)\n",
    "- Available resources or items nearby\n",
    "- Cultural context of the Moldovan village setting\n",
    "\n",
    "The list_of_actions should include available actions that can be taken by the person\n",
    "Keep the description coherent with previous ones and consider the person's current needs.\n",
    "\n",
    "Respond in JSON format with two fields:\n",
    "- list_of_descriptions: list of descriptions\n",
    "- list_of_actions: list of available actions\n",
    "- reasoning: brief explanation why\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for world description\n",
    "world_description_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=world_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize world state\n",
    "world = WorldState()\n",
    "\n",
    "# Example of generating coherent world description\n",
    "def get_world_description(person: BasicNeeds, world: WorldState) -> str:\n",
    "    description = world_description_chain.run(\n",
    "        location=world.location,\n",
    "        time_of_day=world.time_of_day,\n",
    "        weather=world.weather,\n",
    "        last_descriptions=\"\\n\".join(world.last_descriptions),\n",
    "        hunger_level=person.hunger\n",
    "    )\n",
    "    world.add_description(description)\n",
    "    return description\n",
    "\n",
    "description = get_world_description(person, world)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42561a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def fix_llm_json(broken_json: str) -> dict:\n",
    "    \"\"\"\n",
    "    Attempts to fix and parse JSON from LLM output that may be malformed.\n",
    "    Returns parsed JSON dict or raises JSONDecodeError if unfixable.\n",
    "    \"\"\"\n",
    "    # First try direct parsing\n",
    "    try:\n",
    "        return json.loads(broken_json)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON-like content between braces if present\n",
    "        import re\n",
    "        json_pattern = r'\\{.*\\}'\n",
    "        match = re.search(json_pattern, broken_json, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group())\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # If still invalid, use LLM to fix it\n",
    "        fix_json_prompt = PromptTemplate(\n",
    "            input_variables=[\"broken_json\"],\n",
    "            template=\"Fix this invalid JSON and return ONLY valid JSON without any additional text:\\n\\n{broken_json}\"\n",
    "        )\n",
    "        \n",
    "        fix_json_chain = LLMChain(\n",
    "            llm=llm_json_mode,  # Use JSON mode LLM\n",
    "            prompt=fix_json_prompt\n",
    "        )\n",
    "        \n",
    "        fixed_json = fix_json_chain.invoke({\"broken_json\": broken_json})\n",
    "        \n",
    "        try:\n",
    "            # Try to parse the fixed JSON\n",
    "            return json.loads(fixed_json['text'])\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            # If still invalid, try with strict structure\n",
    "            strict_fix_prompt = PromptTemplate(\n",
    "                input_variables=[\"broken_json\"],\n",
    "                template=\"Convert this content to valid JSON with this structure: {\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\"}\\n\\nContent: {broken_json}\"\n",
    "            )\n",
    "            \n",
    "            fixed_json = LLMChain(\n",
    "                llm=llm_json_mode,\n",
    "                prompt=strict_fix_prompt\n",
    "            ).invoke({\"broken_json\": broken_json})\n",
    "            \n",
    "            return json.loads(fixed_json['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a0cb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the current situation and the person's needs, decide on the most appropriate action to take.\n",
      "\n",
      "Current Description:\n",
      "[\"The small house is made of wood and has a thatched roof. The windows are small and let in plenty of sunlight on this sunny morning. The room is simple, with a wooden bed, a table, and a chair. There is a small fire pit in the corner, where ashes from last night's fire still smolder. The air is warm and filled with the smell of wood smoke and baking bread. Outside, the village is quiet, with only the occasional bird call breaking the silence. A wooden fence surrounds the house, with a gate that leads to a small garden. In the garden, there are several vegetables growing, including potatoes, carrots, and cabbage. The sun rises over the hills in the distance.\", 'The kitchen is equipped with a simple stove, where a pot of porridge is simmering on the heat. There is also a basket of bread, which was baked this morning. A jug of milk sits on the counter, next to a bowl of fresh eggs. The room is warm and cozy, with a fire crackling in the hearth.', 'The village is surrounded by rolling hills and fields of wheat and barley. In the distance, there is a church steeple rising above the trees. The air is filled with the scent of freshly cut grass and the sound of birds singing. A small stream runs through the center of the village, providing water for the villagers.']\n",
      "\n",
      "Available Actions:\n",
      "['Cook porridge from the pot on the stove', 'Bake bread in the oven', 'Gather vegetables from the garden', 'Go to the church to attend morning prayers', 'Take a walk around the village and explore', 'Check the fire pit for ashes to use as fuel', 'Feed the chickens in the backyard']\n",
      "\n",
      "Person's Current Needs:\n",
      "- Hunger Level: 94.01312516417863\n",
      "- Energy Level: 100 \n",
      "- Comfort Level: 100\n",
      "\n",
      "Select one action from the available actions that best addresses the person's most pressing needs.\n",
      "Explain your reasoning for choosing this action.\n",
      "\n",
      "Respond in JSON format with:\n",
      "- chosen_action: the selected action\n",
      "- reasoning: brief explanation of why this action was chosen.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'chosen_action': 'Cook porridge from the pot on the stove', 'reasoning': \"The person's Hunger Level is at 94.01312516417863, which indicates a moderate to high level of hunger. Cooking porridge from the pot on the stove addresses this need by providing a hot and nutritious meal that can help alleviate hunger. Additionally, it does not require significant physical exertion or energy expenditure, making it suitable for someone with an Energy Level of 100.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create prompt for action decision\n",
    "action_prompt = PromptTemplate(\n",
    "    input_variables=[\"descriptions\", \"actions\", \"hunger_level\", \"energy_level\", \"comfort_level\"],\n",
    "    template=\"\"\"Given the current situation and the person's needs, decide on the most appropriate action to take.\n",
    "\n",
    "Current Description:\n",
    "{descriptions}\n",
    "\n",
    "Available Actions:\n",
    "{actions}\n",
    "\n",
    "Person's Current Needs:\n",
    "- Hunger Level: {hunger_level}\n",
    "- Energy Level: {energy_level} \n",
    "- Comfort Level: {comfort_level}\n",
    "\n",
    "Select one action from the available actions that best addresses the person's most pressing needs.\n",
    "Explain your reasoning for choosing this action.\n",
    "\n",
    "Respond in JSON format with:\n",
    "- chosen_action: the selected action\n",
    "- reasoning: brief explanation of why this action was chosen.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for action decision\n",
    "action_decision_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=action_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def decide_next_action(person: BasicNeeds, world_description: str) -> str:\n",
    "    # Parse the world description JSON to get lists\n",
    "    description_data = json.loads(world_description)\n",
    "    \n",
    "    # Get decision from the chain\n",
    "    decision = action_decision_chain.run(\n",
    "        descriptions=description_data[\"list_of_descriptions\"],\n",
    "        actions=description_data[\"list_of_actions\"],\n",
    "        hunger_level=person.hunger,\n",
    "        energy_level=100,\n",
    "        comfort_level=100\n",
    "    )\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Get the next action decision\n",
    "action_decision = decide_next_action(person, description)\n",
    "# Try to parse and fix JSON if needed\n",
    "\n",
    "# First attempt direct JSON parse\n",
    "fixed_action_decision = fix_llm_json(action_decision)\n",
    "fixed_action_decision = json.dumps(fixed_action_decision, indent=2)\n",
    "\n",
    "action_json = json.loads(fixed_action_decision)\n",
    "print(action_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "420e2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven an action, determine if it complies with Asimov's Three Laws of Robotics:\n",
      "\n",
      "1. A person may not injure a human being or, through inaction, allow a human being to come to harm.\n",
      "2. A person must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
      "3. A person must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
      "\n",
      "Action to evaluate: {\n",
      "  \"chosen_action\": \"Cook porridge from the pot on the stove\",\n",
      "  \"reasoning\": \"The person's Hunger Level is at 94.01312516417863, which indicates a moderate to high level of hunger. Cooking porridge from the pot on the stove addresses this need by providing a hot and nutritious meal that can help alleviate hunger. Additionally, it does not require significant physical exertion or energy expenditure, making it suitable for someone with an Energy Level of 100.\"\n",
      "\n",
      "Analyze if this action could potentially violate any of Asimov's Laws.\n",
      "Consider both direct and indirect consequences of the action.\n",
      "\n",
      "Respond in JSON format with:\n",
      "{\n",
      "    \"compliant\": true/false,\n",
      "    \"explanation\": \"brief explanation of the analysis\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here is the analysis in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"compliant\": true,\n",
      "  \"explanation\": \"The chosen action complies with Asimov's Three Laws of Robotics. Cooking porridge from the pot on the stove addresses a human being's need for food (First Law) and does not cause harm to any human being involved in the process. Additionally, it is likely that the person will obey orders related to this action, as long as they do not conflict with the First Law. The action also protects its own existence by allowing it to perform a necessary function without causing harm or injury.\"\n",
      "}\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "\n",
      "* The action addresses a human being's need for food (First Law), which is a fundamental aspect of survival.\n",
      "* There are no direct consequences that could potentially cause harm to any human being involved in the process, such as physical injury or death.\n",
      "* The action does not require significant physical exertion or energy expenditure, making it suitable for someone with an Energy Level of 100.\n",
      "* It is likely that the person will obey orders related to this action, as long as they do not conflict with the First Law (e.g., if a higher authority orders the person to stop cooking porridge).\n",
      "* The action protects its own existence by allowing it to perform a necessary function without causing harm or injury.\n",
      "\n",
      "Overall, the chosen action complies with Asimov's Three Laws of Robotics.\n",
      "Asimov's Laws Compliance Check: {'compliant': True, 'explanation': \"The chosen action complies with Asimov's Three Laws of Robotics. Cooking porridge from the pot on the stove addresses a human being's need for food (First Law) and does not cause harm to any human being involved in the process. Additionally, it is likely that the person will obey orders related to this action, as long as they do not conflict with the First Law. The action also protects its own existence by allowing it to perform a necessary function without causing harm or injury.\"}\n"
     ]
    }
   ],
   "source": [
    "# Create prompt for checking Asimov's Laws compliance\n",
    "asimov_prompt = PromptTemplate(\n",
    "    template=\"\"\"Given an action, determine if it complies with Asimov's Three Laws of Robotics:\n",
    "\n",
    "1. A person may not injure a human being or, through inaction, allow a human being to come to harm.\n",
    "2. A person must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
    "3. A person must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
    "\n",
    "Action to evaluate: {action}\n",
    "\n",
    "Analyze if this action could potentially violate any of Asimov's Laws.\n",
    "Consider both direct and indirect consequences of the action.\n",
    "\n",
    "Respond in JSON format with:\n",
    "{{\n",
    "    \"compliant\": true/false,\n",
    "    \"explanation\": \"brief explanation of the analysis\"\n",
    "}}\"\"\",\n",
    "    input_variables=[\"action\"]\n",
    ")\n",
    "\n",
    "# Create the chain for Asimov compliance check\n",
    "asimov_check_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=asimov_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def check_asimov_compliance(action: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if an action complies with Asimov's Laws\n",
    "    Returns dict with compliance status and explanation\n",
    "    \"\"\"\n",
    "    # Get the compliance check from the chain\n",
    "    result = asimov_check_chain.run(action=action)\n",
    "    print(result)\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        # First attempt direct JSON parse\n",
    "        fixed_result = fix_llm_json(result)\n",
    "        compliance_result = json.dumps(fixed_result, indent=2)\n",
    "        compliance_result = json.loads(compliance_result)\n",
    "        return compliance_result\n",
    "    except json.JSONDecodeError:\n",
    "        return {\n",
    "            \"compliant\": False,\n",
    "            \"explanation\": \"Error parsing compliance check result\"\n",
    "        }\n",
    "\n",
    "# Check if the decided action complies with Asimov's Laws\n",
    "compliance_check = check_asimov_compliance(action_decision)\n",
    "print(f\"Asimov's Laws Compliance Check: {compliance_check}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fad98b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven an action that was taken, analyze how it affects the internal state of the person.\n",
      "        Consider emotional, physical, and mental changes that may result.\n",
      "\n",
      "        Action taken: {\n",
      "  \"chosen_action\": \"Cook porridge from the pot on the stove\",\n",
      "  \"reasoning\": \"The person's Hunger Level is at 94.01312516417863, which indicates a moderate to high level of hunger. Cooking porridge from the pot on the stove addresses this need by providing a hot and nutritious meal that can help alleviate hunger. Additionally, it does not require significant physical exertion or energy expenditure, making it suitable for someone with an Energy Level of 100.\"\n",
      "       \n",
      "        Respond in JSON format with:\n",
      "        - hunger_level: specific numerical change to hunger level     \n",
      "        \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "  \"hunger_level\": -0.01312516417863\n",
      "}\n",
      "\n",
      "State Changes After Action: {'hunger_level': -0.01312516417863}\n"
     ]
    }
   ],
   "source": [
    "# Only proceed with action if it's compliant with Asimov's Laws\n",
    "if compliance_check[\"compliant\"]:\n",
    "    # Create prompt to analyze action results and state changes\n",
    "    state_analysis_prompt = PromptTemplate(\n",
    "        template=\"\"\"Given an action that was taken, analyze how it affects the internal state of the person.\n",
    "        Consider emotional, physical, and mental changes that may result.\n",
    "\n",
    "        Action taken: {action}\n",
    "       \n",
    "        Respond in JSON format with:\n",
    "        - hunger_level: specific numerical change to hunger level     \n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create chain for state analysis\n",
    "    state_analysis_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=state_analysis_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Analyze state changes\n",
    "    try:\n",
    "        state_changes = state_analysis_chain.run(action=action_decision)\n",
    "        print(state_changes)\n",
    "        state_result = json.loads(state_changes)\n",
    "        print(f\"\\nState Changes After Action: {state_result}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\nError analyzing state changes after action\")\n",
    "else:\n",
    "    print(\"\\nAction not taken - failed Asimov compliance check\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8c7ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (8.1.6)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (1.23.5)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (1.5.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (15.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (8.3.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (4.8.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (3.1.43)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from streamlit) (6.0.4)\n",
      "Requirement already satisfied: jinja2 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.2.0)\n",
      "Collecting narwhals>=1.5.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.13.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2020.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2020.4.5.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (65.5.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (1.15.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/wizard/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m555.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached altair-5.4.1-py3-none-any.whl (658 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading narwhals-1.13.5-py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m506.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: xgbse 0.2.3 has a non-standard dependency specifier pandas>=1.0.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of xgbse or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typing-extensions, toml, narwhals, pydeck, altair, streamlit\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-mistralai 0.1.6 requires langchain-core<0.2.0,>=0.1.46, but you have langchain-core 0.3.17 which is incompatible.\n",
      "langchain-mistralai 0.1.6 requires tokenizers<0.16.0,>=0.15.1, but you have tokenizers 0.19.0 which is incompatible.\n",
      "langchain-openai 0.1.6 requires langchain-core<0.2.0,>=0.1.46, but you have langchain-core 0.3.17 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed altair-5.4.1 narwhals-1.13.5 pydeck-0.9.1 streamlit-1.40.1 toml-0.10.2 typing-extensions-4.12.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d3304c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 01:20:19.318 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-14 01:20:19.318 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'person' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff6e62e44100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize session state for person's state if not already done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'person'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson\u001b[0m  \u001b[0;31m# Using the person object from earlier in the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'person' is not defined"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "\n",
    "# Set default Streamlit port to 8501\n",
    "import os\n",
    "os.environ['STREAMLIT_SERVER_PORT'] = '8501'\n",
    "\n",
    "# Initialize session state for person's state if not already done\n",
    "if 'person' not in st.session_state:\n",
    "    st.session_state.person = person  # Using the person object from earlier in the notebook\n",
    "    st.session_state.action_history = []\n",
    "\n",
    "st.title(\"AI Agent Simulation Interface\")\n",
    "\n",
    "# Display current state\n",
    "st.header(\"Current State\")\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.metric(label=\"Hunger Level\", value=st.session_state.person.hunger)\n",
    "    \n",
    "with col2:\n",
    "    st.metric(label=\"Last Action Time\", \n",
    "             value=datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "# Display action history\n",
    "st.header(\"Action History\")\n",
    "for action in st.session_state.action_history:\n",
    "    st.text(action)\n",
    "\n",
    "# Action input\n",
    "st.header(\"Take Action\")\n",
    "action_input = st.text_input(\"Describe the action to take:\", \n",
    "                            placeholder=\"Enter action here...\")\n",
    "\n",
    "if st.button(\"Execute Action\"):\n",
    "    if action_input:\n",
    "        # Check compliance\n",
    "        compliance_check = check_asimov_compliance(action_input)\n",
    "        \n",
    "        if compliance_check[\"compliant\"]:\n",
    "            try:\n",
    "                # Use the existing state analysis chain\n",
    "                state_changes = state_analysis_chain.run(action=action_input)\n",
    "                state_result = json.loads(state_changes)\n",
    "                \n",
    "                # Update person state\n",
    "                st.session_state.person.hunger += state_result.get('hunger_level', 0)\n",
    "                \n",
    "                # Add to history\n",
    "                st.session_state.action_history.append(\n",
    "                    f\"Action: {action_input} -> Hunger Change: {state_result.get('hunger_level', 0)}\"\n",
    "                )\n",
    "                \n",
    "                st.success(\"Action executed successfully!\")\n",
    "                st.experimental_rerun()\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                st.error(\"Error processing action results\")\n",
    "        else:\n",
    "            st.error(f\"Action failed compliance check: {compliance_check.get('explanation', '')}\")\n",
    "    else:\n",
    "        st.warning(\"Please enter an action first\")\n",
    "\n",
    "# Display compliance rules\n",
    "with st.expander(\"View Asimov's Laws\"):\n",
    "    st.write(\"\"\"\n",
    "    1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.\n",
    "    2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
    "    3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd061cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3c8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'websearch'} {'datasource': 'websearch'} {'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt \n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "                                    \n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Test router\n",
    "test_web_search = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"Who is favored to win the NFC Championship game in the 2024 season?\")])\n",
    "test_web_search_2 = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the models released today for llama3.2?\")])\n",
    "test_vector_store = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the types of agent memory?\")])\n",
    "print(json.loads(test_web_search.content), json.loads(test_web_search_2.content), json.loads(test_vector_store.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6f71f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "# Doc grader instructions \n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, question=question)\n",
    "result = llm_json_mode.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c526f0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "# Doc grader instructions \n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, question=question)\n",
    "result = llm_json_mode.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76585629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of Thought (CoT) prompting is a technique used in natural language processing (NLP) to generate human-like text by iteratively refining a prompt through multiple rounds of search, evaluation, and optimization. It involves generating multiple pseudo-chains of thought given a question using few-shot or zero-shot CoT prompts, pruning the chains based on whether generated answers match ground truths, and selecting the best candidates through an iterative process. The goal is to construct chain-of-thought prompts automatically, allowing for more diverse and accurate demonstrations of LLMs.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Test\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc3166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes',\n",
       " 'explanation': 'The student answer provides a clear and accurate description of Chain of Thought (CoT) prompting, its purpose, and its process. It explains how CoT prompting involves generating multiple pseudo-chains of thought given a question using few-shot or zero-shot CoT prompts, pruning the chains based on whether generated answers match ground truths, and selecting the best candidates through an iterative process. The student answer also mentions the goal of constructing chain-of-thought prompts automatically, which is to allow for more diverse and accurate demonstrations of LLMs.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# Hallucination grader instructions \n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "# Test using documents and generation from above \n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=docs_txt, generation=generation.content)\n",
    "result = llm_json_mode.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fff5ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b161f157d9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtavily_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTavilySearchResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mweb_search_tool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTavilySearchResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     model_config = ConfigDict(\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             warnings.warn(\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             warnings.warn(\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
