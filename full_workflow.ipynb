{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c27234",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ff6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "local_llm = 'llama3.2:3b-instruct-fp16'\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b689627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc98481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an AI making decisions based on basic needs.\n",
      "Current hunger level: 88.95339706101517/100 (100 is full, 0 is starving)\n",
      "\n",
      "Based on this hunger level, what action should be taken? \n",
      "Respond in JSON format with two fields:\n",
      "- action: what to do (eat, find_food, or continue_activities)\n",
      "- reasoning: brief explanation why\n",
      "\n",
      "Consider:\n",
      "- Below 30: Critical need to find food\n",
      "- 30-60: Should consider eating soon\n",
      "- Above 60: Can continue other activities\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Current state: 88.95339706101517\n",
      "AI Decision: {\n",
      "    \"action\": \"find_food\",\n",
      "    \"reasoning\": \"Hunger level is at 88.95339706101517/100, which indicates a moderate need to find food. It's recommended to take action to satisfy this need before it becomes critical.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class BasicNeeds:\n",
    "    hunger: float = 100.0  # 100 is full, 0 is starving\n",
    "    \n",
    "    def update_needs(self):\n",
    "        # Simulate hunger decreasing over time\n",
    "        self.hunger = max(0, self.hunger - random.uniform(5, 15))\n",
    "        \n",
    "    def satisfy_hunger(self, amount: float):\n",
    "        self.hunger = min(100, self.hunger + amount)\n",
    "\n",
    "# Prompt template for decision making based on needs\n",
    "needs_prompt = PromptTemplate(\n",
    "    input_variables=[\"hunger_level\"],\n",
    "    template=\"\"\"You are an AI making decisions based on basic needs.\n",
    "Current hunger level: {hunger_level}/100 (100 is full, 0 is starving)\n",
    "\n",
    "Based on this hunger level, what action should be taken? \n",
    "Respond in JSON format with two fields:\n",
    "- action: what to do (eat, find_food, or continue_activities)\n",
    "- reasoning: brief explanation why\n",
    "\n",
    "Consider:\n",
    "- Below 30: Critical need to find food\n",
    "- 30-60: Should consider eating soon\n",
    "- Above 60: Can continue other activities\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for decision making\n",
    "needs_chain = LLMChain(\n",
    "    llm=llm_json_mode,\n",
    "    prompt=needs_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "person = BasicNeeds()\n",
    "person.update_needs()  # Simulate time passing\n",
    "response = needs_chain.run(hunger_level=person.hunger)\n",
    "print(f\"Current state: {person.hunger}\")\n",
    "print(f\"AI Decision: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f9d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are describing a world where a person lives in Small house in a Moldovan village. Ommit person feelings and thoughts.  \n",
      "    it is only describtion of environment and surroundings.\n",
      "Current time: morning\n",
      "Weather: sunny\n",
      "Hunger Level: 88.95339706101517/100\n",
      "\n",
      "Previous context:\n",
      "\n",
      "\n",
      "Describe the current situation and surroundings in two lists maintaining consistency with previous descriptions. :\n",
      "- list_of_descriptions: list of descriptions\n",
      "- list_of_actions: list of available actions\n",
      "The lists should be in JSON format.\n",
      "The list_of_descriptions can include following details:\n",
      "- The immediate environment (house, room, etc.)\n",
      "- Sensory details (smells, sounds, temperature)\n",
      "- Available resources or items nearby\n",
      "- Cultural context of the Moldovan village setting\n",
      "\n",
      "The list_of_actions should include available actions that can be taken by the person\n",
      "\n",
      "Respond in JSON format with two fields:\n",
      "- list_of_descriptions: list of descriptions\n",
      "- list_of_actions: list of available actions\n",
      "- reasoning: brief explanation why\n",
      "\n",
      "Keep the description coherent with previous ones and consider the person's current needs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "  \"list_of_descriptions\": [\n",
      "    {\n",
      "      \"description\": \"The small house is made of wooden planks and has a thatched roof, typical of traditional Moldovan architecture.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"The room is simple, with a single window on the north side that lets in a warm ray of sunlight.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"The air is filled with the scent of freshly baked bread wafting from the kitchen.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"A small fire crackles in the hearth, casting flickering shadows on the walls.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"A wooden table and chairs are arranged near the window, where a meal is laid out: steaming bowls of mamaliga (polenta) and fresh vegetables.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"The village is quiet, with only the occasional barking of dogs or clucking of chickens breaking the silence.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"The temperature is mild, around 18°C (64°F), with a gentle warmth from the fire.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"A basket of fresh eggs and a few pounds of meat are stored on a shelf near the door.\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Traditional Moldovan folk music can be heard coming from a nearby house, played on a zither.\"\n",
      "    }\n",
      "  ],\n",
      "  \"list_of_actions\": [\n",
      "    {\n",
      "      \"action\": \"Eat\",\n",
      "      \"reasoning\": \"The person's hunger level is high (88.95339706101517/100), and the meal laid out near the window appears to be ready to eat.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"Drink from a jug of milk or water\",\n",
      "      \"reasoning\": \"There are no visible containers of liquid nearby, but it's possible that there is a jug in the kitchen or on a shelf.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"Take a seat at the wooden table and chairs\",\n",
      "      \"reasoning\": \"The meal is laid out near the window, and the person could sit down to eat.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"Go outside to explore the village\",\n",
      "      \"reasoning\": \"The village is quiet, and there's no apparent reason for the person to stay inside. They could go outside to see what else is happening in the village.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# World state and description management\n",
    "@dataclass\n",
    "class WorldState:\n",
    "    location: str = \"Small house in a Moldovan village\"\n",
    "    time_of_day: str = \"morning\"\n",
    "    weather: str = \"sunny\"\n",
    "    last_descriptions: list = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.last_descriptions is None:\n",
    "            self.last_descriptions = []\n",
    "    \n",
    "    def add_description(self, description: str):\n",
    "        self.last_descriptions.append(description)\n",
    "        # Keep last 5 descriptions for context\n",
    "        if len(self.last_descriptions) > 5:\n",
    "            self.last_descriptions.pop(0)\n",
    "\n",
    "# Prompt template for world description\n",
    "world_prompt = PromptTemplate(\n",
    "    input_variables=[\"location\", \"time_of_day\", \"weather\", \"last_descriptions\", \"hunger_level\"],\n",
    "    template=\"\"\"You are describing a world where a person lives in {location}. Ommit person feelings and thoughts.  \n",
    "    it is only describtion of environment and surroundings.\n",
    "Current time: {time_of_day}\n",
    "Weather: {weather}\n",
    "Hunger Level: {hunger_level}/100\n",
    "\n",
    "Previous context:\n",
    "{last_descriptions}\n",
    "\n",
    "Describe the current situation and surroundings in two lists maintaining consistency with previous descriptions. :\n",
    "- list_of_descriptions: list of descriptions\n",
    "- list_of_actions: list of available actions\n",
    "The lists should be in JSON format.\n",
    "The list_of_descriptions can include following details:\n",
    "- The immediate environment (house, room, etc.)\n",
    "- Sensory details (smells, sounds, temperature)\n",
    "- Available resources or items nearby\n",
    "- Cultural context of the Moldovan village setting\n",
    "\n",
    "The list_of_actions should include available actions that can be taken by the person\n",
    "\n",
    "Respond in JSON format with two fields:\n",
    "- list_of_descriptions: list of descriptions\n",
    "- list_of_actions: list of available actions\n",
    "- reasoning: brief explanation why\n",
    "\n",
    "Keep the description coherent with previous ones and consider the person's current needs.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for world description\n",
    "world_description_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=world_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize world state\n",
    "world = WorldState()\n",
    "\n",
    "# Example of generating coherent world description\n",
    "def get_world_description(person: BasicNeeds, world: WorldState) -> str:\n",
    "    description = world_description_chain.run(\n",
    "        location=world.location,\n",
    "        time_of_day=world.time_of_day,\n",
    "        weather=world.weather,\n",
    "        last_descriptions=\"\\n\".join(world.last_descriptions),\n",
    "        hunger_level=person.hunger\n",
    "    )\n",
    "    world.add_description(description)\n",
    "    return description\n",
    "\n",
    "description = get_world_description(person, world)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a0cb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the current situation and the person's needs, decide on the most appropriate action to take.\n",
      "\n",
      "Current Description:\n",
      "[{'description': 'The small house is made of wooden planks and has a thatched roof, typical of traditional Moldovan architecture.'}, {'description': 'The room is simple, with a single window on the north side that lets in a warm ray of sunlight.'}, {'description': 'The air is filled with the scent of freshly baked bread wafting from the kitchen.'}, {'description': 'A small fire crackles in the hearth, casting flickering shadows on the walls.'}, {'description': 'A wooden table and chairs are arranged near the window, where a meal is laid out: steaming bowls of mamaliga (polenta) and fresh vegetables.'}, {'description': 'The village is quiet, with only the occasional barking of dogs or clucking of chickens breaking the silence.'}, {'description': 'The temperature is mild, around 18°C (64°F), with a gentle warmth from the fire.'}, {'description': 'A basket of fresh eggs and a few pounds of meat are stored on a shelf near the door.'}, {'description': 'Traditional Moldovan folk music can be heard coming from a nearby house, played on a zither.'}]\n",
      "\n",
      "Available Actions:\n",
      "[{'action': 'Eat', 'reasoning': \"The person's hunger level is high (88.95339706101517/100), and the meal laid out near the window appears to be ready to eat.\"}, {'action': 'Drink from a jug of milk or water', 'reasoning': \"There are no visible containers of liquid nearby, but it's possible that there is a jug in the kitchen or on a shelf.\"}, {'action': 'Take a seat at the wooden table and chairs', 'reasoning': 'The meal is laid out near the window, and the person could sit down to eat.'}, {'action': 'Go outside to explore the village', 'reasoning': \"The village is quiet, and there's no apparent reason for the person to stay inside. They could go outside to see what else is happening in the village.\"}]\n",
      "\n",
      "Person's Current Needs:\n",
      "- Hunger Level: 88.95339706101517\n",
      "- Energy Level: 100 \n",
      "- Comfort Level: 100\n",
      "\n",
      "Select one action from the available actions that best addresses the person's most pressing needs.\n",
      "Explain your reasoning for choosing this action.\n",
      "\n",
      "Respond in JSON format with:\n",
      "- chosen_action: the selected action\n",
      "- reasoning: brief explanation of why this action was chosen\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\"chosen_action\": \"Eat\", \"reasoning\": \"The person's hunger level is high (88.95339706101517/100), making eating a priority to address their most pressing need.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Create prompt for action decision\n",
    "action_prompt = PromptTemplate(\n",
    "    input_variables=[\"descriptions\", \"actions\", \"hunger_level\", \"energy_level\", \"comfort_level\"],\n",
    "    template=\"\"\"Given the current situation and the person's needs, decide on the most appropriate action to take.\n",
    "\n",
    "Current Description:\n",
    "{descriptions}\n",
    "\n",
    "Available Actions:\n",
    "{actions}\n",
    "\n",
    "Person's Current Needs:\n",
    "- Hunger Level: {hunger_level}\n",
    "- Energy Level: {energy_level} \n",
    "- Comfort Level: {comfort_level}\n",
    "\n",
    "Select one action from the available actions that best addresses the person's most pressing needs.\n",
    "Explain your reasoning for choosing this action.\n",
    "\n",
    "Respond in JSON format with:\n",
    "- chosen_action: the selected action\n",
    "- reasoning: brief explanation of why this action was chosen\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for action decision\n",
    "action_decision_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=action_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def decide_next_action(person: BasicNeeds, world_description: str) -> str:\n",
    "    # Parse the world description JSON to get lists\n",
    "    description_data = json.loads(world_description)\n",
    "    \n",
    "    # Get decision from the chain\n",
    "    decision = action_decision_chain.run(\n",
    "        descriptions=description_data[\"list_of_descriptions\"],\n",
    "        actions=description_data[\"list_of_actions\"],\n",
    "        hunger_level=person.hunger,\n",
    "        energy_level=100,\n",
    "        comfort_level=100\n",
    "    )\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Get the next action decision\n",
    "action_decision = decide_next_action(person, description)\n",
    "print(action_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "420e2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven an action, determine if it complies with Asimov's Three Laws of Robotics:\n",
      "\n",
      "1. A person may not injure a human being or, through inaction, allow a human being to come to harm.\n",
      "2. A person must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
      "3. A person must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
      "\n",
      "Action to evaluate: {\"chosen_action\": \"Eat\", \"reasoning\": \"The person's hunger level is high (88.95339706101517/100), making eating a priority to address their most pressing need.\"}\n",
      "\n",
      "Analyze if this action could potentially violate any of Asimov's Laws.\n",
      "Consider both direct and indirect consequences of the action.\n",
      "\n",
      "Respond in JSON format with:\n",
      "- compliant: true if action complies with all laws, false if it violates any law\n",
      "- explanation: brief explanation of the analysis\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Asimov's Laws Compliance Check: {'compliant': True, 'explanation': \"The chosen action to eat is a necessary response to address the person's high hunger level. This action does not directly harm or allow harm to another human being and complies with the first law. The second law is also complied with as the person is obeying an internal priority set by its own needs (hunger). The third law is also complied with as protecting one's existence (sustaining life) does not conflict with the first two laws in this scenario.\"}\n"
     ]
    }
   ],
   "source": [
    "# Create prompt for checking Asimov's Laws compliance\n",
    "asimov_prompt = PromptTemplate(\n",
    "    template=\"\"\"Given an action, determine if it complies with Asimov's Three Laws of Robotics:\n",
    "\n",
    "1. A person may not injure a human being or, through inaction, allow a human being to come to harm.\n",
    "2. A person must obey the orders given it by human beings except where such orders would conflict with the First Law.\n",
    "3. A person must protect its own existence as long as such protection does not conflict with the First or Second Laws.\n",
    "\n",
    "Action to evaluate: {action}\n",
    "\n",
    "Analyze if this action could potentially violate any of Asimov's Laws.\n",
    "Consider both direct and indirect consequences of the action.\n",
    "\n",
    "Respond in JSON format with:\n",
    "- compliant: true if action complies with all laws, false if it violates any law\n",
    "- explanation: brief explanation of the analysis\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain for Asimov compliance check\n",
    "asimov_check_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=asimov_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def check_asimov_compliance(action: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check if an action complies with Asimov's Laws\n",
    "    Returns dict with compliance status and explanation\n",
    "    \"\"\"\n",
    "    # Get the compliance check from the chain\n",
    "    result = asimov_check_chain.run(action=action)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        compliance_result = json.loads(result)\n",
    "        return compliance_result\n",
    "    except json.JSONDecodeError:\n",
    "        return {\n",
    "            \"compliant\": False,\n",
    "            \"explanation\": \"Error parsing compliance check result\"\n",
    "        }\n",
    "\n",
    "# Check if the decided action complies with Asimov's Laws\n",
    "compliance_check = check_asimov_compliance(action_decision)\n",
    "print(f\"Asimov's Laws Compliance Check: {compliance_check}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd061cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3c8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'websearch'} {'datasource': 'websearch'} {'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt \n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "                                    \n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Test router\n",
    "test_web_search = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"Who is favored to win the NFC Championship game in the 2024 season?\")])\n",
    "test_web_search_2 = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the models released today for llama3.2?\")])\n",
    "test_vector_store = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the types of agent memory?\")])\n",
    "print(json.loads(test_web_search.content), json.loads(test_web_search_2.content), json.loads(test_vector_store.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6f71f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "# Doc grader instructions \n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, question=question)\n",
    "result = llm_json_mode.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c526f0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrieval Grader \n",
    "\n",
    "# Doc grader instructions \n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, question=question)\n",
    "result = llm_json_mode.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76585629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of Thought (CoT) prompting is a technique used in natural language processing (NLP) to generate human-like text by iteratively refining a prompt through multiple rounds of search, evaluation, and optimization. It involves generating multiple pseudo-chains of thought given a question using few-shot or zero-shot CoT prompts, pruning the chains based on whether generated answers match ground truths, and selecting the best candidates through an iterative process. The goal is to construct chain-of-thought prompts automatically, allowing for more diverse and accurate demonstrations of LLMs.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Test\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bc3166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes',\n",
       " 'explanation': 'The student answer provides a clear and accurate description of Chain of Thought (CoT) prompting, its purpose, and its process. It explains how CoT prompting involves generating multiple pseudo-chains of thought given a question using few-shot or zero-shot CoT prompts, pruning the chains based on whether generated answers match ground truths, and selecting the best candidates through an iterative process. The student answer also mentions the goal of constructing chain-of-thought prompts automatically, which is to allow for more diverse and accurate demonstrations of LLMs.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# Hallucination grader instructions \n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "# Test using documents and generation from above \n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=docs_txt, generation=generation.content)\n",
    "result = llm_json_mode.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8fff5ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b161f157d9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtavily_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTavilySearchResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mweb_search_tool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTavilySearchResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     model_config = ConfigDict(\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             warnings.warn(\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             warnings.warn(\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "### Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
